general:
  print_log_freq: 100
  num_works: 8
  seed: 2021
  grad_acc_step: 1
  fp16: True
  mixup: True
  alpha: 1.0 # hyper-parameter for mixup beta distribution
  train_file: ./input/seti-breakthrough-listen/train_labels.csv
  test_file: ./input/seti-breakthrough-listen/sample_submission.csv
  output_dir: ./exp/exp053
  train: True
  n_fold: 5
  trn_fold:
    - 0
    - 1
    - 2
    - 3
    - 4
  debug: False
  no_mixup_train_epoch: 100000  # 実質, 学習中はずっとmixupをかけ続ける
  tta: True
  class_num: 1

dataset:
  use_axis:
    - 0
    #- 1
    - 2
    #- 3
    - 4
    #- 5
  3_channels: False
  transpose: True
  freq_normalize: False

snapshot:
  enable: True
  cycle: 2

train:
  loss_function: BCEWithLogitsLoss
  patience: 100000
  train_batch_size: 64
  val_batch_size: 128
  num_workers: 8
  epochs: 50
  monitoring: score

scheduler:
  name: CosineAnnealingWarmRestarts
  params: 
    T_0: 50
    T_mult: 1
    eta_min: 0.00000001
    last_epoch: -1

optimizer:
  name: SGD
  params:
    lr: 0.1
    momentum: 0.9
    weight_decay: 0.0001
    nesterov: True


model:
  name: rexnet_130
  pretrained: True
  class_num: 1
  in_channels: 1
  dropout_rate: 0.2
  global_pool_type: avg

test:
  batch_size: 128
# transformsは順番大事
transforms:
  train:
    - name : Resize
      params:
        height: 512
        width: 512
    #- name: RandomBrightnessContrast
    #  params:
    #    p : 0.5
    - name: FreqMask
      params:
        p : 0.5
        max_mask_num: 2
    - name: TimeMask
      params:
        p : 0.5
        max_mask_num: 2
    - name: HorizontalFlip
      params:
        p : 0.5
    #- name: RandamRotate90
    #  params:
    #    p : 0.5
    - name: VerticalFlip
      params:
        p : 0.5
    - name: ToTensorV2
      params:
        p : 1.0
  
  valid:
    - name: Resize
      params:
        height : 512
        width : 512
    - name : ToTensorV2
      params:
        p : 1.0
    

split:
  name: StratifiedKFold
  params:
    n_splits: 5
    random_state: 42
    shuffle: True


lr_range_test:
  trn_fold:
    - 0
    #- 1
    #- 2
    #- 3
    #- 4
  train:
    loss_function: BCEWithLogitsLoss
    patience: 5
    train_batch_size: 64
    val_batch_size: 128
    num_workers: 8
    epochs: 1
    monitoring: score
  optimizer:
    name: AdamW
    params:
      weight_decay: 0.00001
      amsgrad: False
  plot:
    y_axis: AUC

target_cols: 
  - target